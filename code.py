# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qLTQ9cu3UUo2jF7M2GCoP2DG2pOL-kOz
"""

# importing the modules
import requests
from bs4 import BeautifulSoup

import io
import pandas as pd
from google.colab import files
df = files.upload()
# print(df[0])

df = pd.read_excel('Input.xlsx', engine = 'openpyxl')

links = df.values.tolist()
# print (df)
# print(links)
# links = df.tolist()
# links
l1= []
l2 = []
for key in links:
#     print(key[1])
    url = key[1]

# making requests instance
    reqs = requests.get(url, headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'})
  
# using the BeaitifulSoup module
    soup = BeautifulSoup(reqs.text, 'html.parser')
    x = []
    y = []
    for title in soup.find_all('h1'):
        x.append(title.get_text())
    for title in soup.find_all('p'):
        y.append(title.get_text())
    l1.append(x)
    l2.append(y)


# heading = soup.find('h1')

df[2]=l1
df[3]=l2
df[4] = df[2]+df[3]
df[5] = df[4].apply(lambda x: " ".join(x))

import nltk
nltk.download("stopwords")

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.remove('i')
stop_words.remove('we')
stop_words.remove('my')
stop_words.remove('ours')

def convert(lst):
    return ([i for item in lst for i in item.split()])

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 0 and token not in stop_words:
            result.append(token)
            
    return result

df[6] = df[5].apply(preprocess)

for i in range(170):
  df[6][i] = [word.upper() for word in df[6][i]]

for i in range(170):
  df[4][i] = convert(df[4][i])

df[6]

df

dictionary = files.upload()

dictionary = pd.read_excel('LoughranMcDonald_MasterDictionary_2020.xlsx', engine = 'openpyxl')

dictionary

pp = ["I","we","my","ours","us"]

positive = []
negative = []
for ind in dictionary.index:
     if dictionary['Negative'][ind] != 0 :
       negative.append(dictionary['Word'][ind])
     if dictionary['Positive'][ind] != 0 :
       positive.append(dictionary['Word'][ind])

for i in range(170):
  list3 = set(df[6][i])&set(negative)
  df[2][i] = sorted(list3, key = lambda k : df[6][i]. index(k))

for i in range(170):
  list3 = set(df[6][i])&set(positive)
  df[3][i] = sorted(list3, key = lambda k : df[6][i]. index(k))

ans = files.upload()

ans = pd.read_excel('Output Data Structure.xlsx', engine = 'openpyxl')

num1 = [*range(0,170,1)]
for i in range(170):
  num1[i] = len(df[5][i].split())

num1

for i in range(170):
  ans['POSITIVE SCORE'][i] = len(df[3][i])
  ans['NEGATIVE SCORE'][i] = len(df[2][i])
  ans['POLARITY SCORE'][i] = (ans['POSITIVE SCORE'][i]-ans['NEGATIVE SCORE'][i])/((ans['POSITIVE SCORE'][i]+ans['NEGATIVE SCORE'][i])+0.000001)
  ans['SUBJECTIVITY SCORE'][i] = (ans['POSITIVE SCORE'][i]+ans['NEGATIVE SCORE'][i])/((len(df[6][i]))+0.000001)
  ans['WORD COUNT'][i] = len(df[6][i])

numoe = []
numow = []
for item in df[6]:
  n = 0
  numow.append(len(item))
  for item2 in item:
    n = n + len(item2)
  numoe.append(n)
for i in range(170):
  numoe[i] = numoe[i]/numow[i]

numoe

numosa = []
numoc = []
numost = []
for item in df[6]:
  y=0
  z = 0
  numos = []
  for item1 in item:
    x = 0
    for w in item1:
      if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U'):
        x=x+1
    numos.append(x)
    if x>2:
      y=y+1
    if x>0:
      z = z+1
  numosa.append(numos)
  numoc.append(y)
  numost.append(z)

numost

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

numosen = [*range(0,170,1)]
for i in range(170):
  numosen[i] = len(sent_tokenize(df[5][i]))

numosen

pp = [word.upper() for word in pp]

for i in range(170):
  df[4][i] = [word.upper() for word in df[4][i]]

for i in range(170):
  list3 = set(df[4][i])&set(pp)
  df[2][i] = sorted(list3, key = lambda k : df[4][i]. index(k))
  ans['PERSONAL PRONOUNS'][i] = len(df[2][i])
  ans['AVG WORD LENGTH'][i] = numoe[i]
  ans['SYLLABLE PER WORD'][i] = numost[i]/num1[i]
  ans['COMPLEX WORD COUNT'][i] = numoc[i]
  ans['AVG SENTENCE LENGTH'][i] = num1[i]/numosen[i]
  ans['PERCENTAGE OF COMPLEX WORDS'][i] = numoc[i]/num1[i]
  ans['FOG INDEX'][i] = 0.4 * (ans['AVG SENTENCE LENGTH'][i]+ans['PERCENTAGE OF COMPLEX WORDS'][i])
  ans['AVG NUMBER OF WORDS PER SENTENCE'][i] = num1[i]/numosen[i]

ans

ans.to_csv('file1.csv')

file_name = 'Output Data Structure.xlsx'
ans.to_excel(file_name)